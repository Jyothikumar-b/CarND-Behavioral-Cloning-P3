{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Behavioral Cloning** \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Behavioral Cloning Project**\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "* Use the simulator to collect data of good driving behavior\n",
    "* Build, a convolution neural network in Keras that predicts steering angles from images\n",
    "* Train and validate the model with a training and validation set\n",
    "* Test that the model successfully drives around track one without leaving the road\n",
    "* Summarize the results with a written report\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rubric Points\n",
    "### Here I will consider the [rubric points](https://review.udacity.com/#!/rubrics/432/view) individually and describe how I addressed each point in my implementation.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Submitted & Code Quality\n",
    "\n",
    "#### 1. Submission includes all required files and can be used to run the simulator in autonomous mode\n",
    "My project includes the following files:\n",
    "* `model.ipynb` containing the script to create and train the model\n",
    "* `drive.py` for driving the car in autonomous mode\n",
    "* `model.h5` containing a trained convolution neural network \n",
    "* `writeup_report.md` summarizing the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Submission includes functional code\n",
    "Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing \n",
    "```sh\n",
    "python drive.py model.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Submission code is usable and readable\n",
    "\n",
    "The `model.ipynb` file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. An appropriate model architecture has been employed\n",
    "\n",
    "My model consists of a convolution neural network with 5x5 filter sizes and depths between 6 and 120 (`model.ipynb` file, cell 12) \n",
    "\n",
    "The model includes LeakyReLU layers to introduce nonlinearity, and the data is normalized in the model using a Keras lambda layer. \n",
    "\n",
    "#### 2. Attempts to reduce overfitting in the model\n",
    "\n",
    "Overfitting is controlled by following steps.\n",
    "* In each convolution layer, I have used *Max Pooling*. It helps in reducing the dimension as well as makes neurans to perform better. \n",
    "* Using data augumentation techniques, I have distributed the training data across all the output class.\n",
    "* In addition to that, the model was trained and validated on different data sets to ensure that the model was not overfitting (code line 10-16). The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.\n",
    "\n",
    "#### 3. Model parameter tuning\n",
    "\n",
    "* `Learning rate` : The model used an adam optimizer, so the learning rate was not tuned manually (`model.ipynb` file, cell 12, line 40).\n",
    "\n",
    "#### 4. Appropriate training data\n",
    "\n",
    "I have used Udacity training data. There were **Three** images(Center,Left,Right) for every frame and steering angle for center image. We have *8036* frame details. So, totally there were *24108* images given as input.\n",
    "\n",
    "\n",
    "***Data Distribution Of Given Input Data***\n",
    "\n",
    "![alt_text](./Images/RawInput_Distribution.jpg)\n",
    "\n",
    "From the above graph, it is observed that we didn't have same amount of data in each output classes. we can achieve equal distribution by two ways.\n",
    "1. We can improve the samples for output classes which are lower\n",
    "2. Reducing the samples which has large amount of data\n",
    "\n",
    "I chose the second way. As most of the data has roughly 300 images in an average, increaing these output classes is not the good choice. For the given problem, we don't require these much data also. So, I have selected only maximum of 200 images per output class. Additionaly, I have skipped output classes which has less then 10 images.\n",
    "\n",
    "\n",
    "***Data Distribution Of Selected Data***\n",
    "\n",
    "![alt_text](./Images/SelectedInput_DD.jpg)\n",
    "\n",
    "The above data is comparatively well distributed. Agin, this is not evenly distributed in all output classes. As we don't take large turn everytime. Mostly we drive straightly with slight turn. So, these selected data will work without any issue.\n",
    "\n",
    "I have used a combination of central, left and right images for training my model. This will help in recovering from the left and right sides of the road. \n",
    "\n",
    "For details about how I created the training data, see the next section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture and Training Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Solution Design Approach\n",
    "I have divided the problem into Data Augumentation & Building the neural network. For each change in the data set, I will check my model on different model architecture. From each set, minimum one model will be selected for further improvement.\n",
    "\n",
    "***SET 1 :***\n",
    "![alt_text](./Images/SET_1_Summary.jpg)\n",
    "\n",
    "***SET 2***\n",
    "![alt](./Images/SET_2_Summary.jpg)\n",
    "\n",
    "***SET 3***\n",
    "![alt](./Images/SET_3_Summary.jpg)\n",
    "\n",
    "***SET 4***\n",
    "![alt](./Images/SET_4_Summary.jpg)\n",
    "> **Note :** For each SET seperate python notebook is used. These notebooks are also uploaded with results for reference.(For example : `SET 1` uses `1_Mode_Training.ipynb` respectively)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Final Model Architecture\n",
    "My final model consists of **Four** hidden layers.( 3 Convolution layer followed by one dense layer)\n",
    "\n",
    "| Layer         \t\t|     Description\t        \t\t\t\t\t| \n",
    "|:---------------------:|:---------------------------------------------:| \n",
    "| Input         \t\t| 160x320x3    \t\t\t\t\t\t            | \n",
    "| Resizing Image        | 85x320x3                                      |\n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 81x316x6 \t|\n",
    "| Leaky ReLU Activation\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride,  2x2 filter, outputs 40x158x6\t\t|\n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 36x154x36 \t|\n",
    "| Leaky ReLU Activation\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride,  2x2 filter, outputs 18x77x3\t    |\n",
    "| Convolution 5x5     \t| 1x1 stride, valid padding, outputs 14x73x120 \t|\n",
    "| Leaky ReLU Activation\t\t\t\t\t\t\t\t\t\t\t\t\t|\n",
    "| Max pooling\t      \t| 2x2 stride,  2x2 filter, outputs 7x36x120\t    |\n",
    "| Fully connected#1\t\t| 30240 input, 256 output\t\t\t\t        |\n",
    "| Fully connected#2\t\t| 256 input, 1 output     \t\t\t\t        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Creation of the Training Set & Training Process\n",
    "**Training Set Selection :**\n",
    "\n",
    "As discussed in the previous section, apart from 24108 training images 10818 images selected. Among them 20 percent of the images are used for validation.\n",
    "The input data is distributed among all output classes to avoid biased output. The whole data set is shuffled to get random classes in each batch.\n",
    "![alt](./Images/Y_Distribution.jpg)\n",
    "\n",
    "**Data Augumentation :**\n",
    "\n",
    "The upper portion of the image not required for detecting the lanes. so, we are slicing the images in the following way. This will reduce the computation cost as well as increase the accuracy.\n",
    "> Input Image :\n",
    ">> ![alt](./Images/center_2016_12_01_13_30_48_287.jpg)\n",
    "\n",
    "> Output Image :\n",
    ">> ![alt](./Images/Cropped.jpg)\n",
    "\n",
    "The cropped image is normalized using the below formulae:\n",
    "```python\n",
    ">> x=(x/255.0)-0.5\n",
    "```\n",
    "**Training Process :**\n",
    "\n",
    "* Among 80% of input data is taken for training and remaining 20% for validation.\n",
    "* A batch of 32 augumented image is evaluated by my model\n",
    "* The loss will be calculated using `Mean Square Error` function.\n",
    "* Depending upon the loss, `Adam optimizer` will update the weights by back propogation algorithm\n",
    "* This process is continued for all the batches in our training data. Then, the model is evaluated against the validation data\n",
    "\n",
    "The whole training process is repeated for 20 cycle (Epochs). I am plotting the Epochs Vs Loss function to understand the behaviour of my model\n",
    "\n",
    "> ![alt](./Graphs/4_Model_5.png) \n",
    ">>Red line  : Validation loss\n",
    "\n",
    ">>Blue line : Training loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
